{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdEdRVsf/Q3iZoqxXHjWTv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 랜덤 포레스트\n","\n","### 정형 데이터\n","어떤 구조로 되어 있다는 뜻, CSV나 데이터 베이스, 엑셀에 저장하기 쉽다.\n","\n","### 비정형 데이터\n","텍스트 데이터, 디지털 음악, 사진 등이 비정형 데이터에 속한다.\n","\n","정형 데이터를 다루는 데 가장 뛰어난 성과를 보이는 알고리즘이 앙상블 학습이다.\n","이 알고리즘은 대부분 결정 트리를 기반으로 만들어져 있다.\n","\n","비정형 데이터는 어떤 알고리즘을 사용해야할까?\n","신경망 알고리즘을 주로 사용한다.\n","\n","### 랜덤 포레스트\n","\n","랜덤 포레스트는 앙상블 학습의 대표 주자 중 하나로 안정적인 성능 덕분에 널리 사용된다.\n","랜덤 포레스트는 결정 트리를 랜덤하게 만들어 결정 트리의 숲을 만든다.\n","그리고 각 결정 트리의 예측을 사용하여 최종 예측을 만든다.\n","\n","랜덤 포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데, 이데이터를 만드는 방식이 독특하다.\n","우리가 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터륾 만든다.\n","\n","1000 개의 샘플이 들어있는 가방에서 100개의 샘플을 뽑는다면 먼저 1개를 뽑고, 뽑았던 1개를 다시 가방에 넣는다.\n","이런식으로 계속 해서 100개를 가방에서 뽑으면 중복된 샘플을 뽑을 수 있다.\n","이렇게 만들어진 샘플을 부트스트랩 샘플이라고 부른다.\n","\n","또한 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른다음 이 중에서 최선의 분할을 찾는다.\n","\n","분류 모델인 RandomForestClassifier 는 기본적으로 전체 특성 개수의 제곱근 만큼의 특성을 선택한다.\n","\n","즉 4개의 특성이 있다면 노드마다 2개를 랜덤하게 선택하여 사용한다.\n","\n","다만 회귀 모델인 RandomForestRegressor 는 전체 특성을 사용한다.\n","\n","사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련한다.\n","\n","그 다음 분류 시에는 각 트리의 클래스별 확률을 평군하여 가장 높은 확률을 가진 클래스를 예측으로 삼는다.\n","회귀일 대는 단순히 각 트리의 예측을 평균한다.\n","\n","\n","랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다.\n","종종 기본 매개변수 설정 만으로도 아주 좋은 결과를 낸다.\n","\n"],"metadata":{"id":"zfi_-NhMWlnm"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","wine = pd.read_csv('https://bit.ly/wine_csv_data')\n","# wine.head()\n","data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n","target = wine['class'].to_numpy()\n","train_input, test_input, train_target, test_target = train_test_split(\n","    data, target, test_size=0.2, random_state=42\n",")"],"metadata":{"id":"NnHrvarNYyIi","executionInfo":{"status":"ok","timestamp":1717507215642,"user_tz":-540,"elapsed":274,"user":{"displayName":"Marmin","userId":"14871703657352502094"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["cross_validate() 함수를 사용하여 교차 검증을 수행,\n","RandomForestClassifier는 기본적으로 100개의 결정 트리를 사용하므로 n_jobs 매개변수를 -1 로 지정하여 모든 CPU 코어를 사용하는 것이 좋다.\n","\n","return_train_score 매개변수를 True로 지정하면 검증 점수뿐 아니라 훈련 세트에 대한 점수도 같이 반환된다.\n","훈련세트와 검증 세트의 점수를 비교하면 과대적합을 파악하는 데 용이하다."],"metadata":{"id":"GBMx8L9yaGAW"}},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmXuuP9LWiNg","executionInfo":{"status":"ok","timestamp":1717507547072,"user_tz":-540,"elapsed":2123,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"2892f38f-42d2-4101-fdbc-1cc387e99696"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.9973541965122431 0.8905151032797809\n"]}],"source":["from sklearn.model_selection import cross_validate\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n","scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"]},{"cell_type":"markdown","source":["랜덤 포레스트는 결정 트리의 앙상블이기때문에 DecisionTreeClassifier가 제공하는 중요한 매개변수를 모두 제공한다.\n","\n","criterion, max_depth, max_features, min_samples_split, min_impurity_decrease, min_samples_leaf 등이다. 또한 결정 트리의 큰 장점 중 하나인 특성 중요도를 계산한다.\n","\n","랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것이다.\n"],"metadata":{"id":"6WJF0f5sbeoG"}},{"cell_type":"code","source":["rf.fit(train_input, train_target)\n","print(rf.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9v5Sm73beF_","executionInfo":{"status":"ok","timestamp":1717507735203,"user_tz":-540,"elapsed":1269,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"e1025e61-eb46-41cf-95d1-305ecf9b2453"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.23167441 0.50039841 0.26792718]\n"]}]},{"cell_type":"markdown","source":["각각 [알코올 도수, 당도, pJ] 였는데, 두번째 특성인 당도의 중요도가 감소하고 알코올 도수와 pH 특성의 중요도가 조금 상승했다.\n","\n","이유는 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정 트리를 훈련하기 때문이다.\n","그 결과 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻는다.\n","\n","그 결과 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻는다.\n","이는 과대적합을 줄이고 일반화 성능을 높이는 데 도움이 된다.\n","\n","RandomForestClassifier에는 기능이 하나 더있는데, 자체적으로 모델을 평가하는 점수를 얻을 수 있다.\n","랜덤 포레스트는 훈련 세트에서 중복을 허용하여 부트스트랩 샘플을 만들어 결정 트리를 훈련한다.\n","이때 부트스트랩 샘플에 포함되지 않고 남는 샘플이 있는데 이를 OOB (Out Of Bag) 라 한다.\n","\n","이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정트리를 평가가 가능하다.\n","\n"],"metadata":{"id":"fVrxYRdxcJ9G"}},{"cell_type":"code","source":["rf = RandomForestClassifier(oob_score=True, n_jobs = -1, random_state=42)\n","rf.fit(train_input, train_target)\n","print(rf.oob_score_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r90ttkU6dTIl","executionInfo":{"status":"ok","timestamp":1717508108312,"user_tz":-540,"elapsed":1454,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"932a8899-cd66-41ad-ec75-561f64fa7c37"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8934000384837406\n"]}]},{"cell_type":"markdown","source":["교차 검증에서 얻은 점수와 매우 비슷한 결과를 얻었다.\n","OOB 점수를 사용하면 교차 검증을 대신할 수 있어서 결과적으로 훈련 세트에 더 많은 샘플을 사용이 가능하다.\n"],"metadata":{"id":"iyxSdV1gdyH7"}},{"cell_type":"markdown","source":["# 엑스트라 트리\n","엑스트라 트리는 랜덤 포레스트와 매우 비슷하게 동작한다. 기본적으로 100개의 결정 트리를 훈련한다.\n","랜덤 포레스트와 동일하게 결정 트리가 제공하는 대부분의 매개변수를 지원한다.\n","또한 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는데 사용한다.\n","\n","랜덤 포레스트와 엑스트라 트리의 차이점은 부트스트랩 샘플을 사용하지 않는 점이다.\n","즉각 결정 트리를 만들 때 전체 훈련세트를 사용한다.\n","대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다.\n","\n","하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지지만, 많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다.\n","사이킷런에서 제공하는 엑스트라 트리는 ExtraTreesClassifier이다.\n"],"metadata":{"id":"pJqJMAZneRmA"}},{"cell_type":"code","source":["from sklearn.ensemble import ExtraTreesClassifier\n","\n","et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n","scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=1)\n","\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhfWr3ahfGHj","executionInfo":{"status":"ok","timestamp":1717508647712,"user_tz":-540,"elapsed":3907,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"d1d43301-ba61-4599-ce52-16df580a6bc8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9974503966084433 0.8887848893166506\n"]}]},{"cell_type":"markdown","source":["랜덤 포레스트와 비슷한 결과를 얻었다. 특성이 많지 않아 두 모델의 차이가 크지 않다.\n","보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야한다.\n","랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 엑스트라 트리의 장점이다.\n","\n","엑스트라 트리도 랜덤 포레스트와 마찬가지로 특성 중요도를 제공한다.\n","순서는 [알코올 도수, 당도, pH]인데, 결과를 보면 엑스트라 트리도 결정 트리보다 당도에 대한 의존성이 작다.\n"],"metadata":{"id":"GhisEhxSftn-"}},{"cell_type":"code","source":["et.fit(train_input, train_target)\n","print(et.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThJvYCl4ftbW","executionInfo":{"status":"ok","timestamp":1717508810832,"user_tz":-540,"elapsed":1258,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"b1824ccc-9a5e-4d9c-ea85-d94e98efdfb7"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.20183568 0.52242907 0.27573525]\n"]}]},{"cell_type":"markdown","source":["엑스트라 트리의 회귀버전은 ExtraTreeRegressor 클래스이다.\n"],"metadata":{"id":"AhsGYZWfgMfI"}},{"cell_type":"markdown","source":["# 그레이디언트 부스팅\n","\n","그레이디언트 부스팅 gradient boosting은 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방식이다.\n","\n","사이킷런의 GradientBoostingClassifier는 기본적으로 깊이가 3인 결정트리를 100개 사용한다.\n","깊이가 얕은 결정 트리를 사용하기 떄문에 과대 적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다.\n","\n","그레이디언트란 경사 하강법을 사용하여 트리를 앙상블에 추가한다. 분류에서는 로지스틱 손실 함수를 사용하고 회귀에서는 평균 제곱 오차 함수를 사용한다.\n","\n","경사 하강법은 손실 함수를 산으로 정의하고 가장 낮은 곳을 찾아 내려오는 과정이다.\n","\n","이때 가장 낮은 곳을 찾아 내려오는 방법은 모델의 가중치와 절편을 조금씩 바꾸는 것이다.\n","\n","그레이디언트 부스팅은 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동한다.\n","또 학습률 매개 변수로 속도를 조절한다.\n"],"metadata":{"id":"aZJ4vZTrgx0A"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","gb = GradientBoostingClassifier(random_state=42)\n","scores = cross_validate(\n","    gb, train_input, train_target,\n","    return_train_score=True, n_jobs=-1\n",")\n","print(np.mean(scores['train_score']),np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9E0urITthsZh","executionInfo":{"status":"ok","timestamp":1717509342825,"user_tz":-540,"elapsed":5364,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"18b0aeab-3187-467c-e5a2-3d12528526dc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8881086892152563 0.8720430147331015\n"]}]},{"cell_type":"markdown","source":["그레이디언트 부스팅은 결정 트리의 개수를 늘려도 과대적합에 매우 강하다.\n","학습률을 증가시키고 트리의 개수를 늘리면 성능이 향상된다."],"metadata":{"id":"BygXegA7iQTW"}},{"cell_type":"code","source":["gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n","\n","scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n","\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QoqKnOh4iQAh","executionInfo":{"status":"ok","timestamp":1717509506238,"user_tz":-540,"elapsed":7909,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"be72f1ba-3d15-4e83-ddb9-c0c63839ce3e"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9464595437171814 0.8780082549788999\n"]}]},{"cell_type":"markdown","source":["결정 트리 개수를 500개로 5배나 늘렸지만 과대적합을 잘 억제하고 있다.\n","학습률 learning_rate의 기본값은 0.1이다.\n","그레이디언트 부스팅도 특성 중요도를 제공한다.\n","결과에서 볼 수 있듯이 그레이디언트 부스팅이 랜덤 포레스트보다 더 당도에 집중한다."],"metadata":{"id":"7esjhdsOi-O0"}},{"cell_type":"code","source":["gb.fit(train_input, train_target)\n","print(gb.feature_importances_)\n","gb.score(test_input, test_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvxbmdQpjGZi","executionInfo":{"status":"ok","timestamp":1717509929896,"user_tz":-540,"elapsed":3252,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"03d62431-d0a6-452b-8cad-2a8be3534fa5"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.15872278 0.68010884 0.16116839]\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8707692307692307"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["트리 훈련에 사용할 훈련 세트의 비율을 정하는 subsample 있다.\n","이 매개변수의 기본값은 0.1로 전체 훈련 세트를 사용한다.\n","하지만 subsample이 1보다 작으면 훈련 세트의 일부를 사용한다.\n","이는 마치 경사 하강법 단계마다 일부 샘플을 랜덤하게 선택하여 진행하는 확률적 경사 하강법이나 미니배치 경사 하강법과 비슷하다.\n","\n","일반적으로 그레이디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있다.\n","\n","하지만 순서대로 트리를 추가하기때문에 훈련속도가 느리다.\n","\n","즉 GradientBoostingClassifier에는 n_jobs 매개변수가 없다.\n","그레이디언트 부스팅의 회귀 버전은 GradientBoostingRegressor이다.\n","\n"],"metadata":{"id":"6neKEmqljdv_"}},{"cell_type":"markdown","source":[],"metadata":{"id":"oVT-h3QCkmZd"}},{"cell_type":"markdown","source":["### 히스토그램 기반 그레이디언트 부스팅\n","\n","히스토그램 기반 그레이디언트 부스팅은 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘이다.\n","히스토그램 기반 그레이디언트 부스팅은 먼저 입력 특성을 256개의 구간으로 나눈다.\n","\n","따라서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다.\n","히스토그램 기반 그레이디언트 부스팅은 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용된다.\n","\n","HisGraidientBoostingClassifier을 사용하고, 기본 매개변수에서 안정적인 성능을 얻을 수 있다.\n","해당 클래스에서는 트리의 개수를 지정하는 대신에 max_iter를 사용하여 부스팅 반복 횟수를 지정한다.\n","\n","와인 데이터셋에 HistGradientBoostingClassifier 클래스를 적용하자.\n","사이킷런의 히스토그램 기반 그레이디언트 부스팅은 아직 테스트 과정중이다.\n"],"metadata":{"id":"uw41fnURkzYp"}},{"cell_type":"code","source":["from sklearn.experimental import enable_hist_gradient_boosting\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","\n","hgb = HistGradientBoostingClassifier(random_state=42)\n","scores = cross_validate(hgb, train_input, train_target, return_train_score=True)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TsdIuaIZljIb","executionInfo":{"status":"ok","timestamp":1717510346622,"user_tz":-540,"elapsed":2216,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"afa32e79-740b-4ec7-c89b-b8ed80b70669"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["0.9321723946453317 0.8801241948619236\n"]}]},{"cell_type":"markdown","source":["과대적합을 잘 억제하면서 그레이디언트 부스팅보다 더 높은 성능이다.\n","히스토그램 기반 그레이디언트 부스팅의 특성 중요도를 계한하기 위해 permutation_importance 함수를 사용하자.\n","\n","이 함수는 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰하여 어떤 특성이 중요한지를 계산한다.\n","\n","훈련 세트뿐만 아니라 테스트세트에도 적용이 가능하고, 사이킷런에서 제공하는 추정기 모델에 모두 사용이 가능하다.\n","\n","히스토그램 기반 그레이디언트 부스팅 모델을 훈련하고 훈련세트에서 특성 중요도를 계산해보자.\n","n_repaets 매개변수는 랜덤하게 섞을 횟수를 지정한다.\n","기본값은 5이다."],"metadata":{"id":"1lttmtlbmHxJ"}},{"cell_type":"code","source":["from sklearn.inspection import permutation_importance\n","\n","hgb.fit(train_input, train_target)\n","result = permutation_importance(\n","    hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1\n",")\n","print(result.importances_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1aM388TmsUM","executionInfo":{"status":"ok","timestamp":1717510783926,"user_tz":-540,"elapsed":14697,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"6f74170e-a384-4a30-fbf8-3a73116d4de0"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.08876275 0.23438522 0.08027708]\n","[0.08876275 0.23438522 0.08027708]\n"]}]},{"cell_type":"markdown","source":["permutation_importance 함수가 반환하는 객체는 반복하여 얻은 특성 중요도, 평균, 표준편차를 담고 있다.\n","평균을 출력해보면 랜덤 포레스트와 비슷한 비율이다."],"metadata":{"id":"Cto19sRrn3Z8"}},{"cell_type":"code","source":["result = permutation_importance(hgb, test_input, test_target,\n","                                 n_repeats=10, random_state=42, n_jobs=-1)\n","print(result.importances_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dzuJvy1OoGdt","executionInfo":{"status":"ok","timestamp":1717510959357,"user_tz":-540,"elapsed":1057,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"1ad1cdae-ee4f-4ba3-957e-0dd30769f58c"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.05969231 0.20238462 0.049     ]\n"]}]},{"cell_type":"markdown","source":["테스트 세트의 결과를 보면 그레이디언트 부스팅과 비슷하게 조금 더 당도에 집중하고 있다.\n","이런 분석을 통해 모델을 실전에 투입했을 때 어떤 특성에 관심을 둘지 예상이 가능하다.\n"],"metadata":{"id":"WuzR3eF1ozZl"}},{"cell_type":"code","source":["hgb.score(test_input, test_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJge8jC-o-0T","executionInfo":{"status":"ok","timestamp":1717511135551,"user_tz":-540,"elapsed":8,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"59a8b067-47b2-46e3-d647-45e69007636e"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8723076923076923"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### XGBoost를 사용"],"metadata":{"id":"RlN1irthpI1l"}},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","xgb = XGBClassifier(tree_method='hist', random_state=42)\n","scores = cross_validate(xgb, train_input, train_target, return_train_score=True)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMq4qg3fpMvG","executionInfo":{"status":"ok","timestamp":1717511272739,"user_tz":-540,"elapsed":1375,"user":{"displayName":"Marmin","userId":"14871703657352502094"}},"outputId":"542aa09d-6169-426e-f01d-3c747da5003f"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9558403027491312 0.8782000074035686\n"]}]},{"cell_type":"markdown","source":["히스토그램 기반 그레이디언트 부스팅 라이브러리 마이크로소프트의 LightGBM\n"],"metadata":{"id":"uWN9wShqpuoM"}},{"cell_type":"code","source":["from lightgbm import LGBMClassifier\n","\n","lgb = LGBMClassifier(random_state=42)\n","scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)"],"metadata":{"id":"XGW-goOHp66U","executionInfo":{"status":"ok","timestamp":1717511419814,"user_tz":-540,"elapsed":6460,"user":{"displayName":"Marmin","userId":"14871703657352502094"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["print(np.mean(scores['train_score']),np.mean(scores['test_scores']))"],"metadata":{"id":"TnI0j61-qKXe"},"execution_count":null,"outputs":[]}]}